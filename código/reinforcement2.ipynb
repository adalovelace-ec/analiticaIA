{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2329aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final learned Q-values (Deep Q-Network):\n",
      "[[1.394 1.306 1.255]\n",
      " [0.921 1.02  1.167]\n",
      " [1.022 1.088 1.052]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_40864\\1155754181.py:77: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  states_tensor = torch.tensor([one_hot(s, n_states) for s in states], dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# --- ENVIRONMENT SETUP ---\n",
    "np.random.seed(2)\n",
    "n_states = 3\n",
    "n_actions = 3\n",
    "\n",
    "# Transition probability matrices\n",
    "P = np.zeros((n_actions, n_states, n_states))\n",
    "for a in range(n_actions):\n",
    "    for s in range(n_states):\n",
    "        row = np.random.rand(n_states)\n",
    "        P[a][s] = row / row.sum()\n",
    "\n",
    "# Reward matrix\n",
    "R = np.random.uniform(0, 2, size=(n_states, n_actions))\n",
    "\n",
    "# --- (1) GENERATE EXPERIENCES ---\n",
    "def generate_experiences(P, R, n_states, n_actions, num_episodes=500, max_steps=10):\n",
    "    experiences = []\n",
    "    for _ in range(num_episodes):\n",
    "        s = np.random.randint(0, n_states)\n",
    "        for _ in range(max_steps):\n",
    "            a = np.random.randint(0, n_actions)\n",
    "            s_prime = np.random.choice(n_states, p=P[a][s])\n",
    "            r = R[s, a]\n",
    "            experiences.append((s, a, r, s_prime))\n",
    "            s = s_prime\n",
    "    return experiences\n",
    "\n",
    "experiences = generate_experiences(P, R, n_states, n_actions)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- (2) DEFINE DQN ---\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def one_hot(s, n_states):\n",
    "    vec = np.zeros(n_states)\n",
    "    vec[s] = 1.0\n",
    "    return vec\n",
    "\n",
    "# --- HYPERPARAMETERS ---\n",
    "gamma = 0.95\n",
    "alpha = 0.001\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "# Initialize networks\n",
    "policy_net = DQN(n_states, n_actions)\n",
    "target_net = DQN(n_states, n_actions)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=alpha)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Replay buffer\n",
    "memory = deque(experiences, maxlen=10000)\n",
    "\n",
    "# --- TRAINING LOOP ---\n",
    "for epoch in range(epochs):\n",
    "    batch = random.sample(memory, batch_size)\n",
    "    states, actions, rewards, next_states = zip(*batch)\n",
    "\n",
    "    states_tensor = torch.tensor([one_hot(s, n_states) for s in states], dtype=torch.float32)\n",
    "    next_states_tensor = torch.tensor([one_hot(s_, n_states) for s_ in next_states], dtype=torch.float32)\n",
    "    actions_tensor = torch.tensor(actions, dtype=torch.long).unsqueeze(1)\n",
    "    rewards_tensor = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    q_values = policy_net(states_tensor).gather(1, actions_tensor)\n",
    "    next_q_values = target_net(next_states_tensor).max(1, keepdim=True)[0].detach()\n",
    "    target = rewards_tensor + gamma * next_q_values\n",
    "\n",
    "    loss = loss_fn(q_values, target)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "# --- FINAL Q-VALUES ---\n",
    "final_q_values = policy_net(torch.eye(n_states)).detach().numpy()\n",
    "print(\"Final learned Q-values (Deep Q-Network):\")\n",
    "print(np.round(final_q_values, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "258ac0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q*(0,0) = 1.3938 | Bellman RHS = 1.3748 | Residual = 0.0191\n",
      "Q*(0,1) = 1.3057 | Bellman RHS = 1.5621 | Residual = -0.2564\n",
      "Q*(0,2) = 1.2554 | Bellman RHS = 1.9094 | Residual = -0.6539\n",
      "Q*(1,0) = 0.9207 | Bellman RHS = 2.1026 | Residual = -1.1819\n",
      "Q*(1,1) = 1.0205 | Bellman RHS = 1.5306 | Residual = -0.5102\n",
      "Q*(1,2) = 1.1674 | Bellman RHS = 2.4014 | Residual = -1.2339\n",
      "Q*(2,0) = 1.0221 | Bellman RHS = 2.0943 | Residual = -1.0722\n",
      "Q*(2,1) = 1.0882 | Bellman RHS = 2.1813 | Residual = -1.0930\n",
      "Q*(2,2) = 1.0519 | Bellman RHS = 1.8937 | Residual = -0.8419\n"
     ]
    }
   ],
   "source": [
    "# Verificar Bellman para todos los (s, a)\n",
    "def check_bellman_equation(Q_star, R, P, gamma=0.95):\n",
    "    n_states, n_actions = R.shape\n",
    "    residuals = np.zeros_like(Q_star)\n",
    "\n",
    "    for s in range(n_states):\n",
    "        for a in range(n_actions):\n",
    "            rhs = R[s, a] + gamma * np.dot(P[a][s], np.max(Q_star, axis=1))\n",
    "            lhs = Q_star[s, a]\n",
    "            residuals[s, a] = lhs - rhs\n",
    "            print(f\"Q*({s},{a}) = {lhs:.4f} | Bellman RHS = {rhs:.4f} | Residual = {lhs - rhs:.4f}\")\n",
    "\n",
    "    return residuals\n",
    "\n",
    "# Ejecutar verificaci√≥n\n",
    "residuals = check_bellman_equation(final_q_values, R, P, gamma=0.95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a29e1c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Q* (approx):\n",
      " [[20.08  20.8   20.681]\n",
      " [21.01  20.907 21.816]\n",
      " [21.205 21.174 21.205]]\n",
      "\n",
      "Verifying Bellman Equation:\n",
      "Q*(0,0) = 20.0796 | Bellman RHS = 20.2077 | Residual = -0.1281 as % of Q* = -0.64%\n",
      "Q*(0,1) = 20.7999 | Bellman RHS = 20.7673 | Residual = 0.0326 as % of Q* = 0.16%\n",
      "Q*(0,2) = 20.6809 | Bellman RHS = 20.6492 | Residual = 0.0317 as % of Q* = 0.15%\n",
      "Q*(1,0) = 21.0101 | Bellman RHS = 21.1448 | Residual = -0.1347 as % of Q* = -0.64%\n",
      "Q*(1,1) = 20.9073 | Bellman RHS = 20.8440 | Residual = 0.0633 as % of Q* = 0.30%\n",
      "Q*(1,2) = 21.8158 | Bellman RHS = 21.8041 | Residual = 0.0117 as % of Q* = 0.05%\n",
      "Q*(2,0) = 21.2047 | Bellman RHS = 21.3607 | Residual = -0.1559 as % of Q* = -0.74%\n",
      "Q*(2,1) = 21.1740 | Bellman RHS = 21.2459 | Residual = -0.0718 as % of Q* = -0.34%\n",
      "Q*(2,2) = 21.2052 | Bellman RHS = 21.2316 | Residual = -0.0264 as % of Q* = -0.12%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# --- ENVIRONMENT SETUP ---\n",
    "np.random.seed(2)\n",
    "n_states = 3\n",
    "n_actions = 3\n",
    "\n",
    "# Transition probability matrices\n",
    "P = np.zeros((n_actions, n_states, n_states))\n",
    "for a in range(n_actions):\n",
    "    for s in range(n_states):\n",
    "        row = np.random.rand(n_states)\n",
    "        P[a][s] = row / row.sum()\n",
    "\n",
    "# Reward matrix\n",
    "R = np.random.uniform(0, 2, size=(n_states, n_actions))\n",
    "\n",
    "# --- EXPERIENCE GENERATION ---\n",
    "def generate_experiences(P, R, n_states, n_actions, num_episodes=2000, max_steps=10):\n",
    "    experiences = []\n",
    "    for _ in range(num_episodes):\n",
    "        s = np.random.randint(0, n_states)\n",
    "        for _ in range(max_steps):\n",
    "            a = np.random.randint(0, n_actions)\n",
    "            s_prime = np.random.choice(n_states, p=P[a][s])\n",
    "            r = R[s, a]\n",
    "            experiences.append((s, a, r, s_prime))\n",
    "            s = s_prime\n",
    "    return experiences\n",
    "\n",
    "experiences = generate_experiences(P, R, n_states, n_actions)\n",
    "\n",
    "# --- DQN MODEL ---\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def one_hot(s, n_states):\n",
    "    vec = np.zeros(n_states)\n",
    "    vec[s] = 1.0\n",
    "    return vec\n",
    "\n",
    "# --- HYPERPARAMETERS ---\n",
    "gamma = 0.95\n",
    "alpha = 0.005\n",
    "batch_size = 64\n",
    "epochs = 1000\n",
    "\n",
    "# Initialize networks\n",
    "policy_net = DQN(n_states, n_actions)\n",
    "target_net = DQN(n_states, n_actions)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=alpha)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Replay buffer\n",
    "memory = deque(experiences, maxlen=10000)\n",
    "\n",
    "# --- TRAINING LOOP ---\n",
    "for epoch in range(epochs):\n",
    "    batch = random.sample(memory, batch_size)\n",
    "    states, actions, rewards, next_states = zip(*batch)\n",
    "\n",
    "    states_tensor = torch.tensor([one_hot(s, n_states) for s in states], dtype=torch.float32)\n",
    "    next_states_tensor = torch.tensor([one_hot(s_, n_states) for s_ in next_states], dtype=torch.float32)\n",
    "    actions_tensor = torch.tensor(actions, dtype=torch.long).unsqueeze(1)\n",
    "    rewards_tensor = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    q_values = policy_net(states_tensor).gather(1, actions_tensor)\n",
    "    next_q_values = target_net(next_states_tensor).max(1, keepdim=True)[0].detach()\n",
    "    target = rewards_tensor + gamma * next_q_values\n",
    "\n",
    "    loss = loss_fn(q_values, target)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "# --- FINAL Q* ---\n",
    "Q_star = policy_net(torch.eye(n_states)).detach().numpy()\n",
    "print(\"Final Q* (approx):\\n\", np.round(Q_star, 3))\n",
    "\n",
    "# --- BELLAMAN VERIFICATION ---\n",
    "def check_bellman_equation(Q_star, R, P, gamma=0.95):\n",
    "    print(\"\\nVerifying Bellman Equation:\")\n",
    "    n_states, n_actions = R.shape\n",
    "    residuals = np.zeros_like(Q_star)\n",
    "    for s in range(n_states):\n",
    "        for a in range(n_actions):\n",
    "            rhs = R[s, a] + gamma * np.dot(P[a][s], np.max(Q_star, axis=1))\n",
    "            lhs = Q_star[s, a]\n",
    "            residuals[s, a] = lhs - rhs\n",
    "            print(f\"Q*({s},{a}) = {lhs:.4f} | Bellman RHS = {rhs:.4f} | Residual = {lhs - rhs:.4f} as % of Q* = {(lhs - rhs) / lhs * 100:.2f}%\")\n",
    "    return residuals\n",
    "\n",
    "residuals = check_bellman_equation(Q_star, R, P, gamma=0.95)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural_net1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
