{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "descripción de los datos .. _breast_cancer_dataset:\n",
      "\n",
      "Breast cancer wisconsin (diagnostic) dataset\n",
      "--------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 569\n",
      "\n",
      ":Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      ":Attribute Information:\n",
      "    - radius (mean of distances from center to points on the perimeter)\n",
      "    - texture (standard deviation of gray-scale values)\n",
      "    - perimeter\n",
      "    - area\n",
      "    - smoothness (local variation in radius lengths)\n",
      "    - compactness (perimeter^2 / area - 1.0)\n",
      "    - concavity (severity of concave portions of the contour)\n",
      "    - concave points (number of concave portions of the contour)\n",
      "    - symmetry\n",
      "    - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "    The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "    worst/largest values) of these features were computed for each image,\n",
      "    resulting in 30 features.  For instance, field 0 is Mean Radius, field\n",
      "    10 is Radius SE, field 20 is Worst Radius.\n",
      "\n",
      "    - class:\n",
      "            - WDBC-Malignant\n",
      "            - WDBC-Benign\n",
      "\n",
      ":Summary Statistics:\n",
      "\n",
      "===================================== ====== ======\n",
      "                                        Min    Max\n",
      "===================================== ====== ======\n",
      "radius (mean):                        6.981  28.11\n",
      "texture (mean):                       9.71   39.28\n",
      "perimeter (mean):                     43.79  188.5\n",
      "area (mean):                          143.5  2501.0\n",
      "smoothness (mean):                    0.053  0.163\n",
      "compactness (mean):                   0.019  0.345\n",
      "concavity (mean):                     0.0    0.427\n",
      "concave points (mean):                0.0    0.201\n",
      "symmetry (mean):                      0.106  0.304\n",
      "fractal dimension (mean):             0.05   0.097\n",
      "radius (standard error):              0.112  2.873\n",
      "texture (standard error):             0.36   4.885\n",
      "perimeter (standard error):           0.757  21.98\n",
      "area (standard error):                6.802  542.2\n",
      "smoothness (standard error):          0.002  0.031\n",
      "compactness (standard error):         0.002  0.135\n",
      "concavity (standard error):           0.0    0.396\n",
      "concave points (standard error):      0.0    0.053\n",
      "symmetry (standard error):            0.008  0.079\n",
      "fractal dimension (standard error):   0.001  0.03\n",
      "radius (worst):                       7.93   36.04\n",
      "texture (worst):                      12.02  49.54\n",
      "perimeter (worst):                    50.41  251.2\n",
      "area (worst):                         185.2  4254.0\n",
      "smoothness (worst):                   0.071  0.223\n",
      "compactness (worst):                  0.027  1.058\n",
      "concavity (worst):                    0.0    1.252\n",
      "concave points (worst):               0.0    0.291\n",
      "symmetry (worst):                     0.156  0.664\n",
      "fractal dimension (worst):            0.055  0.208\n",
      "===================================== ====== ======\n",
      "\n",
      ":Missing Attribute Values: None\n",
      "\n",
      ":Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      ":Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      ":Donor: Nick Street\n",
      "\n",
      ":Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      ".. dropdown:: References\n",
      "\n",
      "  - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction\n",
      "    for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on\n",
      "    Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "    San Jose, CA, 1993.\n",
      "  - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and\n",
      "    prognosis via linear programming. Operations Research, 43(4), pages 570-577,\n",
      "    July-August 1995.\n",
      "  - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "    to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994)\n",
      "    163-171.\n",
      "\n",
      "X_test Shape: (143, 30)\n",
      "y_test Shape: (143,)\n",
      "\n",
      "iteracion: 0, loss:  0.9508\n",
      "iteracion: 7000, loss:  0.7588\n",
      "iteracion: 14000, loss:  0.6291\n",
      "iteracion: 21000, loss:  0.5398\n",
      "iteracion: 28000, loss:  0.4758\n",
      "iteracion: 35000, loss:  0.4282\n",
      "iteracion: 42000, loss:  0.3914\n",
      "iteracion: 49000, loss:  0.3622\n",
      "iteracion: 56000, loss:  0.3385\n",
      "iteracion: 63000, loss:  0.3187\n",
      " accuracy: 0.8881\n",
      "Execution time: 20.786771774291992 seconds\n"
     ]
    }
   ],
   "source": [
    "# creamos el modelo (imput, output size, forward pass)\n",
    "# creamos loss functions, optimizer\n",
    "# training loop\n",
    "#    - forward pass\n",
    "#    - backward pass (gradients)\n",
    "#    - update weights\n",
    "\n",
    "\n",
    "# ***** sin direccionar datos procesos a GPU *******\n",
    "\n",
    "\n",
    "#conda install scikit-learn\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# the data is coming from the scikit-learn (sklearn) datasets module. \n",
    "# Specifically, the datasets.load_breast_cancer() function is used to load the Breast Cancer Wisconsin (Diagnostic) dataset.\n",
    "\n",
    "\n",
    "\n",
    "# 0) data retrival and data preparation\n",
    "\n",
    "bc = datasets.load_breast_cancer()\n",
    "X, y = bc.data, bc.target\n",
    "n_samples, n_features = X.shape\n",
    "print(f'descripción de los datos {bc.DESCR}')   #to get the description of the data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state = 1234)\n",
    "# dividimos los datos en train y test, 75% train, 25% test\n",
    "\n",
    "sc = StandardScaler()\n",
    "#crea una instancia del escalador que realizará la transformación\n",
    "\n",
    "X_train = sc.fit_transform(X_train)\n",
    "#  calcula la media y desviación estándar de cada característica en el conjunto de entrenamiento X_train \n",
    "# y transforma esos datos para que cada característica tenga media 0 y desviación estándar 1.\n",
    "\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "#aplica la misma transformación al conjunto de prueba X_test, usando la media \n",
    "# y desviación estándar calculadas en X_train, para mantener la consistencia entre ambos conjuntos.\n",
    "\n",
    "print(\"X_test Shape:\", X_test.shape)\n",
    "print(\"y_test Shape:\", y_test.shape)\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "\n",
    "# 1) data preparation for pytorch\n",
    "\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "y_train = y_train.view(y_train.shape[0],1)\n",
    "y_test = y_test.view(y_test.shape[0],1)\n",
    "\n",
    "# model definition\n",
    "# Y= w*x  with sigmoid at the end\n",
    "\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "\n",
    "    def __init__(self, n_input_features):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(n_input_features,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_predicted = torch.sigmoid(self.linear(x))\n",
    "        return y_predicted\n",
    "    \n",
    "model = LogisticRegression(n_features)\n",
    "learning_rate = 0.00001\n",
    "# Binary Cross-Entropy Loss: This loss function measures the difference between the predicted probabilities \n",
    "# (output of the model) and the actual binary labels (0 or 1). It is particularly suitable for tasks where the \n",
    "# output is a probability value between 0 and 1, such as when using a sigmoid activation function in the final layer of the model.\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Stochastic Gradient Descent (SGD): SGD is an optimization algorithm used to update the model's parameters (weights) \n",
    "# based on the gradients of the loss function. It is called \"stochastic\" because it updates the parameters using a randomly selected subset (mini-batch) of the training data, rather than the entire dataset at once.\n",
    "\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_epochs = 70000\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # forward\n",
    "    y_predicted =model(X_train)\n",
    "    loss = criterion(y_predicted, y_train)\n",
    "\n",
    "    # gradient\n",
    "    loss.backward()\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    #zero gradients\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % (num_epochs/10) == 0:\n",
    "\n",
    "\n",
    "        print(f'iteracion: {epoch}, loss: {loss.item(): .4f}')\n",
    "\n",
    "\n",
    "# Run nvidia-smi command and capture its output\n",
    "#result = subprocess.run([\"nvidia-smi\"], capture_output=True, text=True)\n",
    "#print(result.stdout)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test)\n",
    "    y_pred_cls = y_pred.round()\n",
    "    acc = y_pred_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f' accuracy: {acc:.4f}')\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Execution time: {end_time - start_time} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos el modelo (imput, output size, forward pass)\n",
    "# creamos loss functions, optimizer\n",
    "# training loop\n",
    "#    - forward pass\n",
    "#    - backward pass (gradients)\n",
    "#    - update weights\n",
    "\n",
    "# *****  Direccionar datos procesos a GPU *******\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# To make sure your PyTorch model uses the GPU, you need to explicitly move the model and the data it processes to the GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# the data is coming from the scikit-learn (sklearn) datasets module. \n",
    "# Specifically, the datasets.load_breast_cancer() function is used to load the Breast Cancer Wisconsin (Diagnostic) dataset.\n",
    "\n",
    "# 0) data retrival and data preparation\n",
    "\n",
    "bc = datasets.load_breast_cancer()\n",
    "X, y = bc.data, bc.target\n",
    "n_samples, n_features = X.shape\n",
    "# print(bc.DESCR)   #to get the description of the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state = 1234)\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "print(\"X_test Shape:\", X_test.shape)\n",
    "print(\"y_test Shape:\", y_test.shape)\n",
    "\n",
    "\n",
    "# fit_transform. Fit: This part of the process involves calculating the necessary statistics to perform the transformation. \n",
    "# It calculates the mean and standard deviation of the dataset.\n",
    "# Transform: Based on the statistics calculated during the fit stage, this step scales and shifts the data so that it follows a specific distribution \n",
    "# The transform method is used to apply a previously computed transformation to the data. This method does not calculate any new transformation parameters but uses the ones computed from a previous fit or fit_transform operation.\n",
    "\n",
    "\n",
    "\n",
    "# sc.fit_transform(X_train):\n",
    "# This method is used to fit the scaler to the training data and transform the training data in a single step.\n",
    "# It calculates the mean and standard deviation of the training data and uses these values to standardize the training data.\n",
    "# This method should only be used on the training data to avoid data leakage.\n",
    "# sc.transform(X_train):\n",
    "# This method is used to transform the data using the parameters (mean and standard deviation) learned from the training data.\n",
    "# It assumes that the scaler has already been fitted to the training data using sc.fit or sc.fit_transform.\n",
    "# It applies the same scaling transformation to the input data (in this case, X_train) based on the previously calculated mean and standard deviation.\n",
    "\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "y_train = y_train.view(y_train.shape[0],1)\n",
    "y_test = y_test.view(y_test.shape[0],1)\n",
    "\n",
    "#  Each batch of the data needs to be sent to the GPU\n",
    "\n",
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "\n",
    "# model definition\n",
    "# Y= w*x  whit sigmoid at the end\n",
    "\n",
    "# @jitclass not working\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "\n",
    "    def __init__(self, n_input_features):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(n_input_features,1)\n",
    "\n",
    "    # @jit not working\n",
    "    def forward(self, x):\n",
    "        y_predicted = torch.sigmoid(self.linear(x))\n",
    "        return y_predicted\n",
    "    \n",
    "model = LogisticRegression(n_features).to(device)\n",
    "\n",
    "learning_rate = 0.00001\n",
    "# Binary Cross-Entropy Loss: This loss function measures the difference between the predicted probabilities \n",
    "# (output of the model) and the actual binary labels (0 or 1). It is particularly suitable for tasks where the \n",
    "# output is a probability value between 0 and 1, such as when using a sigmoid activation function in the final layer of the model.\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Stochastic Gradient Descent (SGD): SGD is an optimization algorithm used to update the model's parameters (weights) \n",
    "# based on the gradients of the loss function. It is called \"stochastic\" because it updates the parameters using a randomly selected subset (mini-batch) of the training data, rather than the entire dataset at once.\n",
    "\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_epochs = 1000000\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # forward\n",
    "    y_predicted =model(X_train)     # X_train is already on GPU\n",
    "    loss = criterion(y_predicted, y_train)     # y_train is already on GPU\n",
    "\n",
    "    # gradient\n",
    "    loss.backward()\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    #zero gradients\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % (num_epochs/10) == 0:\n",
    "\n",
    "\n",
    "        print(f'iteracion: {epoch}, loss: {loss.item(): .4f}')\n",
    "\n",
    "\n",
    "# Run nvidia-smi command and capture its output\n",
    "result = subprocess.run([\"nvidia-smi\"], capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test)\n",
    "    y_pred_cls = y_pred.round()\n",
    "    acc = y_pred_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f' accuracy: {acc:.4f}')\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Execution time: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Epoch [1/50000], Loss: 0.6340\n",
      "Epoch [500/50000], Loss: 0.5728\n",
      "Epoch [1000/50000], Loss: 0.5291\n",
      "Epoch [1500/50000], Loss: 0.4791\n",
      "Epoch [2000/50000], Loss: 0.5585\n",
      "Epoch [2500/50000], Loss: 0.5187\n",
      "Epoch [3000/50000], Loss: 0.4338\n",
      "Epoch [3500/50000], Loss: 0.4470\n",
      "Epoch [4000/50000], Loss: 0.4568\n",
      "Epoch [4500/50000], Loss: 0.4442\n",
      "Epoch [5000/50000], Loss: 0.4782\n",
      "Epoch [5500/50000], Loss: 0.4180\n",
      "Epoch [6000/50000], Loss: 0.4356\n",
      "Epoch [6500/50000], Loss: 0.3655\n",
      "Epoch [7000/50000], Loss: 0.3742\n",
      "Epoch [7500/50000], Loss: 0.4525\n",
      "Epoch [8000/50000], Loss: 0.3647\n",
      "Epoch [8500/50000], Loss: 0.4115\n",
      "Epoch [9000/50000], Loss: 0.3804\n",
      "Epoch [9500/50000], Loss: 0.3479\n",
      "Epoch [10000/50000], Loss: 0.3314\n",
      "Epoch [10500/50000], Loss: 0.3508\n",
      "Epoch [11000/50000], Loss: 0.2299\n",
      "Epoch [11500/50000], Loss: 0.2882\n",
      "Epoch [12000/50000], Loss: 0.2836\n",
      "Epoch [12500/50000], Loss: 0.2978\n",
      "Epoch [13000/50000], Loss: 0.3273\n",
      "Epoch [13500/50000], Loss: 0.2812\n",
      "Epoch [14000/50000], Loss: 0.2934\n",
      "Epoch [14500/50000], Loss: 0.2683\n",
      "Epoch [15000/50000], Loss: 0.2756\n",
      "Epoch [15500/50000], Loss: 0.3275\n",
      "Epoch [16000/50000], Loss: 0.2632\n",
      "Epoch [16500/50000], Loss: 0.2828\n",
      "Epoch [17000/50000], Loss: 0.2476\n",
      "Epoch [17500/50000], Loss: 0.2422\n",
      "Epoch [18000/50000], Loss: 0.2713\n",
      "Epoch [18500/50000], Loss: 0.2841\n",
      "Epoch [19000/50000], Loss: 0.2400\n",
      "Epoch [19500/50000], Loss: 0.2325\n",
      "Epoch [20000/50000], Loss: 0.2764\n",
      "Epoch [20500/50000], Loss: 0.2267\n",
      "Epoch [21000/50000], Loss: 0.2726\n",
      "Epoch [21500/50000], Loss: 0.1896\n",
      "Epoch [22000/50000], Loss: 0.2606\n",
      "Epoch [22500/50000], Loss: 0.2755\n",
      "Epoch [23000/50000], Loss: 0.2113\n",
      "Epoch [23500/50000], Loss: 0.2497\n",
      "Epoch [24000/50000], Loss: 0.2148\n",
      "Epoch [24500/50000], Loss: 0.1888\n",
      "Epoch [25000/50000], Loss: 0.2567\n",
      "Epoch [25500/50000], Loss: 0.2045\n",
      "Epoch [26000/50000], Loss: 0.2339\n",
      "Epoch [26500/50000], Loss: 0.2412\n",
      "Epoch [27000/50000], Loss: 0.2489\n",
      "Epoch [27500/50000], Loss: 0.2787\n",
      "Epoch [28000/50000], Loss: 0.1896\n",
      "Epoch [28500/50000], Loss: 0.2684\n",
      "Epoch [29000/50000], Loss: 0.1935\n",
      "Epoch [29500/50000], Loss: 0.2402\n",
      "Epoch [30000/50000], Loss: 0.1459\n",
      "Epoch [30500/50000], Loss: 0.1855\n",
      "Epoch [31000/50000], Loss: 0.2692\n",
      "Epoch [31500/50000], Loss: 0.2663\n",
      "Epoch [32000/50000], Loss: 0.2096\n",
      "Epoch [32500/50000], Loss: 0.2291\n",
      "Epoch [33000/50000], Loss: 0.1856\n",
      "Epoch [33500/50000], Loss: 0.1887\n",
      "Epoch [34000/50000], Loss: 0.1832\n",
      "Epoch [34500/50000], Loss: 0.2406\n",
      "Epoch [35000/50000], Loss: 0.2228\n",
      "Epoch [35500/50000], Loss: 0.1933\n",
      "Epoch [36000/50000], Loss: 0.1989\n",
      "Epoch [36500/50000], Loss: 0.2351\n",
      "Epoch [37000/50000], Loss: 0.2346\n",
      "Epoch [37500/50000], Loss: 0.2396\n",
      "Epoch [38000/50000], Loss: 0.1886\n",
      "Epoch [38500/50000], Loss: 0.1289\n",
      "Epoch [39000/50000], Loss: 0.2226\n",
      "Epoch [39500/50000], Loss: 0.2319\n",
      "Epoch [40000/50000], Loss: 0.2324\n",
      "Epoch [40500/50000], Loss: 0.1936\n",
      "Epoch [41000/50000], Loss: 0.1987\n",
      "Epoch [41500/50000], Loss: 0.1601\n",
      "Epoch [42000/50000], Loss: 0.2138\n",
      "Epoch [42500/50000], Loss: 0.2058\n",
      "Epoch [43000/50000], Loss: 0.1878\n",
      "Epoch [43500/50000], Loss: 0.1802\n",
      "Epoch [44000/50000], Loss: 0.1652\n",
      "Epoch [44500/50000], Loss: 0.2075\n",
      "Epoch [45000/50000], Loss: 0.2190\n",
      "Epoch [45500/50000], Loss: 0.2320\n",
      "Epoch [46000/50000], Loss: 0.1390\n",
      "Epoch [46500/50000], Loss: 0.1801\n",
      "Epoch [47000/50000], Loss: 0.1894\n",
      "Epoch [47500/50000], Loss: 0.1745\n",
      "Epoch [48000/50000], Loss: 0.1855\n",
      "Epoch [48500/50000], Loss: 0.1460\n",
      "Epoch [49000/50000], Loss: 0.1395\n",
      "Epoch [49500/50000], Loss: 0.1083\n",
      "Epoch [50000/50000], Loss: 0.1652\n",
      "\n",
      "Accuracy on test set: 0.9091\n",
      "\n",
      "'nvidia-smi' no está disponible. Asegúrate de tener una GPU NVIDIA con drivers instalados.\n",
      "Execution time: 211.64 seconds\n"
     ]
    }
   ],
   "source": [
    "# ***** ENTRENAMIENTO CON BATCHING EN GPU CON Pytorch *******\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "# ---------- Configuración ----------\n",
    "batch_size = 128\n",
    "learning_rate = 1e-5\n",
    "num_epochs = 50000\n",
    "print_every = num_epochs // 100\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# ---------- Selección del dispositivo ----------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ---------- Preparación de los datos ----------\n",
    "bc = datasets.load_breast_cancer()\n",
    "X, y = bc.data, bc.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=1234\n",
    ")\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# Convertir a tensores\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32)).view(-1, 1)\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32)).view(-1, 1)\n",
    "\n",
    "# Crear DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Enviar test data a GPU\n",
    "X_test = X_test.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "# ---------- Definición del modelo ----------\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, n_input_features):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(n_input_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.linear(x))\n",
    "\n",
    "model = LogisticRegression(X_train.shape[1]).to(device)\n",
    "\n",
    "# ---------- Función de pérdida y optimizador ----------\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# ---------- Entrenamiento ----------\n",
    "for epoch in range(num_epochs):\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Enviar batch a GPU, si está disponible\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        # Forward\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "\n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    if (epoch + 1) % print_every == 0 or epoch == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# ---------- Evaluación ----------\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test)\n",
    "    y_pred_cls = y_pred.round()\n",
    "    acc = y_pred_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'\\nAccuracy on test set: {acc:.4f}')\n",
    "\n",
    "try:\n",
    "    result = subprocess.run([\"nvidia-smi\"], capture_output=True, text=True)\n",
    "    print(\"\\nNVIDIA-SMI output:\\n\", result.stdout)\n",
    "except FileNotFoundError:\n",
    "    print(\"\\n'nvidia-smi' no está disponible. Asegúrate de tener una GPU NVIDIA con drivers instalados.\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Execution time: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# más general:\n",
    "\n",
    "class ThreeLayerNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim):\n",
    "        super(ThreeLayerNetwork, self).__init__()\n",
    "        # Capa 1: Transformación lineal + Tanh\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        # Capa 2: Transformación lineal + ReLU\n",
    "        self.layer2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        # Capa 3: Transformación lineal + LogSoftmax (logit)\n",
    "        self.layer3 = nn.Linear(hidden_dim2, output_dim)\n",
    "        # Función de activación final (logit)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Primera capa: Lineal + Tanh\n",
    "        x = torch.tanh(self.layer1(x))\n",
    "        # Segunda capa: Lineal + ReLU\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        # Tercera capa: Lineal + LogSoftmax\n",
    "        x = self.layer3(x)\n",
    "        return self.log_softmax(x)\n",
    "    \n",
    "# Ejemplo de uso\n",
    "# Hiperparámetros\n",
    "input_size = 10\n",
    "hidden1_size = 64\n",
    "hidden2_size = 32\n",
    "output_size = 3\n",
    "\n",
    "# Crear modelo\n",
    "model = ThreeLayerNetwork(\n",
    "    input_dim=input_size,\n",
    "    hidden_dim1=hidden1_size,\n",
    "    hidden_dim2=hidden2_size,\n",
    "    output_dim=output_size\n",
    ")\n",
    "\n",
    "# Ejemplo de entrada\n",
    "x = torch.randn(5, input_size)  # Batch de 5 muestras\n",
    "\n",
    "# Forward pass\n",
    "output = model(x)\n",
    "print(output.shape)  # Salida: torch.Size([5, 3])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural_net1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
