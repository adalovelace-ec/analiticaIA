{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d276f39f",
   "metadata": {},
   "source": [
    "empezamos a trabajar con gradientes  con la función R^3 -> R, z=f(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de268986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  2.,  16., 216.], grad_fn=<PowBackward1>)\n",
      "z1 tensor([  2.,  16., 216.], grad_fn=<PowBackward1>)  y su shape es torch.Size([3])\n",
      "tensor(78., grad_fn=<MeanBackward0>)\n",
      "Primera derivada x.grad: tensor([ 0.6667,  5.3333, 72.0000], grad_fn=<CopyBackwards>)\n",
      "Hessiana completa:\n",
      "tensor([[ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  2.6667,  0.0000],\n",
      "        [ 0.0000,  0.0000, 48.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\econo\\.conda\\envs\\cursopython2\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\engine.cpp:1206.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cálculos con la segunda derivada: tensor([ 0.0000,  2.6667, 48.0000])\n"
     ]
    }
   ],
   "source": [
    "# gradientes\n",
    "import torch\n",
    "x = torch.tensor([1.0, 2, 3], requires_grad=True)  \n",
    "\n",
    "exponent = torch.tensor([1, 2, 3])     # tensor con potencias\n",
    "\n",
    "z1 = torch.pow(x * 2, exponent)        # eleva cada elemento de (x*2) a la potencia correspondiente\n",
    "\n",
    "print(z1)   # si tenemos algo distinto a un escalar, no podra calcular el gradiente\n",
    "print(f\"z1 {z1}  y su shape es {z1.shape}\")     # esto es un vector, no un escalar\n",
    "z = z1.mean()   # esto es un escalar\n",
    "print(z)\n",
    "\n",
    "# Primera derivada dz/dx, creando grafo para derivadas superiores\n",
    "z.backward(create_graph=True)  \n",
    "print(f\"Primera derivada x.grad: {x.grad}\")\n",
    "\n",
    "#otra forma de referirse a la derivada\n",
    "grad = torch.autograd.grad(z, x, create_graph=True)[0]\n",
    "\n",
    "# Problem: calling .backward() twice on the same computation graph without clearing the gradients. \n",
    "# By default, PyTorch accumulates gradients in the .grad attribute of tensors when .backward() is called. \n",
    "# This means that calling .backward() multiple times without zeroing out the gradients can result in incorrect values because the gradients from \n",
    "# the previous call are added to the new gradients.\n",
    "\n",
    "hessian = torch.zeros(x.size(0), x.size(0))  # matriz Hessiana inicializada en ceros\n",
    "\n",
    "for i in range(x.size(0)):\n",
    "    # Calculamos gradiente de grad[i] respecto a x (segunda derivada)\n",
    "    grad2 = torch.autograd.grad(grad[i], x, retain_graph=True)[0]\n",
    "    hessian[i] = grad2\n",
    "\n",
    "print(\"Hessiana completa:\")\n",
    "print(hessian)\n",
    "\n",
    "\n",
    "# Cálculos con la segunda derivada: gradiente de x.grad respecto a x\n",
    "cal_segunda = torch.autograd.grad(outputs=x.grad, inputs=x, grad_outputs=torch.ones_like(x), retain_graph=True)[0]\n",
    "print(f\"cálculos con la segunda derivada: {cal_segunda}\")\n",
    "\n",
    "# En PyTorch, cuando tienes dos funciones f(x) y g(x) que dependen del mismo tensor x\n",
    "# con requires_grad=True, y calculas los gradientes de ambas, los gradientes se acumulan \n",
    "# en el atributo x.grad a menos que los limpies explícitamente\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
